<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="generator" content="Hugo 0.62.2" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/skeleton.css">
    <link rel="stylesheet" href="css/custom.css">
    <link rel="stylesheet" href="css/toc.css">
    <!-- <link rel="alternate" href="index.xml" type="application/rss+xml" title="PopMAG"> -->
    <link rel="shortcut icon" href="favicon.png" type="image/x-icon" />
    <title>Rebuttal | Museformer</title>

    <script>
        window.addEventListener('DOMContentLoaded', () => {

            const observer = new IntersectionObserver(entries => {
                entries.forEach(entry => {
                    const id = entry.target.getAttribute('id');
                    if (entry.intersectionRatio > 0) {
                        document.querySelector(`nav li a[href="#${id}"]`).parentElement.classList.add('active');
                    } else {
                        document.querySelector(`nav li a[href="#${id}"]`).parentElement.classList.remove('active');
                    }
                });
            });

            // Track all sections that have an `id` applied
            document.querySelectorAll('section[id]').forEach((section) => {
                observer.observe(section);
            });

        });
    </script>
</head>

<body>

    <div>

        <header role="banner">

        </header>


        <main role="main">


            <div>
                <article itemscope itemtype="https://schema.org/BlogPosting">
                    <h1 class="entry-title" itemprop="headline">Museformer: Transformer with Fine- and Coarse-Grained
                        Attention for Music Generation</h1>

                    <div align="justify">
                        This page shows supplementary materials for rebuttal.
                    </div>

                    <div align="justify">
                        Click <a href="https://museformer.github.io/">here</a> to go to the original demo page.
                    </div>

                    <section id="introduction">
                        <h2>Introduction</h2>
                        <p>In this page, we will show:</p>
                        <p>- <a href="#dataset_analysis"><b>Similarity Distribution on Different Datasets</b></a>: to show the generalization of the
                            structure-related bar selection to other genres and datasets.</p>
                        <p>- <a href="#music_analysis"><b>Similarity Distribution on Generated Music</b></a>: to provide an objective comparison about
                            music structures among Museformer and the baseline models.</p>
                        <p>- <a href="#ssm"><b>Self-Similarity Matrix of Demos</b></a>: to visualize the music structures of the demos and
                            to demonstrate that Museformer does not simply copy but can generate variations.</p>
                    </section>

                    <section id="dataset_analysis">
                        <h2>Similarity Distribution on Different Datasets</h2>
                        <p>To see the music structures of different genres and datasets and check whether our
                            structure-related bar pattern is widely applicable, we do the similarity statistics: We
                            compute the similarities between each pair of bars within a song, and see the similarity
                            distribution with respect to the distance between two bars. Here, the similarity is defined
                            as the number of the common notes over the size of the union set of notes of the two bars,
                            so its value should range from 0.0 to 1.0. Two notes are considered equal when their onsets
                            within their bars, pitches, and durations are all the same. For each song, we would obtain a
                            self-similarity matrix, whose size is num_bars X num_bars. Then, we compute the similarity
                            distribution with respect to the distance between two bars, which shows the average value of
                            similarities between two bars at each specific distance. The datasets and the results are as
                            follows.</p>

                        <section id="dataset_analysis_lmd">
                            <h3>LMD (TopMAGD)</h3>
                            The widely used MIDI dataset, Lakh MIDI (LMD), contains songs of various genres. The <a
                                href="http://www.ifs.tuwien.ac.at/mir/msd/TopMAGD.html">TopMAGD dataset</a> annotates
                            the genre labels to many songs, a part of which is matched to songs in LMD. There are
                            altogether 13 genres: Pop/Rock, Electronic, Rap, Jazz, Latin, R&B, International, Country,
                            Reggae, Blues, Vocal, Folk, and New Age. We utilize the genre labels provided by TopMAGD to
                            sort the LMD songs, and then do the statistics for each genre. The similarity is computed
                            for the melody track. The results are shown below. Note that we omit the similarity when
                            distance is 0, since it is always equal to 1.

                            These 13 genres should cover most songs we listen to in daily life. As you may see from the
                            figures, for all of the genres, the similarities at distance 1, 2, and multiples of 4 are
                            generally higher than others, which means music tend to repeat or largely refer to previous
                            bars at these distances. This demonstrates our selection of the structure-related bars in
                            the paper, i.e., the previous 1st, 2nd, 4th, 8th, 16th, 24th, 32th bars are selected as the
                            structure-related bars and are directly attended to via the fine-grained attention, is
                            general to various of genres.

                            <figure>
                                <figcaption>Pop Music</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/Pop_Rock-basic-40.png">
                            </figure>

                            <figure>
                                <figcaption>Electronic</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/Electronic-basic-1482-40.png">
                            </figure>

                            <figure>
                                <figcaption>Rap</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/Rap-basic-186-40.png">
                            </figure>

                            <figure>
                                <figcaption>Jazz</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/Jazz-basic-759-40.png">
                            </figure>

                            <figure>
                                <figcaption>Latin</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/Latin-basic-758-40.png">
                            </figure>

                            <figure>
                                <figcaption>R&B</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/RnB-basic-1240-40.png">
                            </figure>

                            <figure>
                                <figcaption>International</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/International-basic-588-40.png">
                            </figure>

                            <figure>
                                <figcaption>Country</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/Country-basic-1152-40.png">
                            </figure>

                            <figure>
                                <figcaption>Reggae</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/Reggae-basic-41-40.png">
                            </figure>

                            <figure>
                                <figcaption>Blues</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/Blues-basic-52-40.png">
                            </figure>

                            <figure>
                                <figcaption>Vocal</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/Vocal-basic-365-40.png">
                            </figure>

                            <figure>
                                <figcaption>Folk</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/Folk-basic-103.png">
                            </figure>

                            <figure>
                                <figcaption>New Age</figcaption>
                                <img src="rebuttal_materials/similarity/lmd/NewAge-basic-258-40.png">
                            </figure>
                        </section>

                        <section id="dataset_analysis_symphony">
                            <h3>Symphony</h3>
                            We also do the same statistics on a large-scale <a
                                href="https://github.com/symphonynet/SymphonyNet">symphony dataset</a>. This dataset is
                            composed of classical and contemporary multi-track symphony performances. Since it is hard
                            to tell which is the melody track of a symphony performance, we use the average similarity
                            over all the tracks. The results are shown in the following figure. It shows that our
                            structure-related bar selection can also be reasonably applicable to this dataset.

                            <figure>
                                <figcaption>Symphony Dataset</figcaption>
                                <img src="rebuttal_materials/similarity/symphony/symphony.png">
                            </figure>
                        </section>

                        <br />
                        In conclusion, through these statistics results, we can see that our structure-related bar
                        selection can generalize to other genres and datasets, and should cover most of the music we
                        involve with in our daily life. Note that we only chose 8 bars as the structure-related bars
                        in our paper is because it is a trade-off between model performances and efficiency. <b>Our
                            high-level idea that fine-grained attention is for important contents and coarse-grained
                            attention is for other contextual information, is universally fittable for music data.
                            Our model can be easily adapted to more structure-related bars or even any other music
                            structures by simply changing a set of hyperparameters in the implementation.</b>
                    </section>

                    <section id="music_analysis">
                        <h2>Similarity Distribution on Generate Music</h2>
                        Using the same method described in <a href="#dataset_analysis">the last section</a>, we compute the bar-pair similarity distribution on 100 music pieces generated by each model (Museformer and the baseline models). The result is as follows:

                        <figure>
                            <figcaption>Music Transformer</figcaption>
                            <img src="rebuttal_materials/similarity_demo/mt_100demos.png">
                        </figure>

                        <figure>
                            <figcaption>Transformer-XL</figcaption>
                            <img src="rebuttal_materials/similarity_demo/xl_100demos.png">
                        </figure>

                        <figure>
                            <figcaption>Longformer</figcaption>
                            <img src="rebuttal_materials/similarity_demo/longformer_100demos.png">
                        </figure>

                        <figure>
                            <figcaption>Linear Transformer</figcaption>
                            <img src="rebuttal_materials/similarity_demo/linear_100demos.png">
                        </figure>

                        <figure>
                            <figcaption>Museformer</figcaption>
                            <img src="rebuttal_materials/similarity_demo/mf_100demos.png">
                        </figure>

                        <figure>
                            <figcaption>Training Data</figcaption>
                            <img src="rebuttal_materials/similarity_demo/training_set.png">
                        </figure>

                        The last figure shows the similarity distribution on the training data. The more the distribution of generated music is like that of the training data, the better. As we can see from the above figures: 1) Music Transformer that is trained with sequences of limited lengths cannot generate music with good structures. 2) Linear Transformer that uses approximated attention cannot well model the music structures even if the linearized attention allows a token to see all of the previous tokens. 3) The generated music pieces by Transformer-XL or Longformer manifest the similar distribution to that of the training data, yet it is still not good enough. 4) It is quite obvious that Museformer can generated music with structures close to the training data. 5) Interestingly, we found that the similarity distribution results perfectly align with the subjective evaluation results over the structures presented in Table 2 of the paper.

                    </section>

                    <section id="ssm">
                        <h2>Self-Similarity Matrix of Demos</h2>
                        
                        Using the same method of similarity computation described <a href="#dataset_analysis">here</a>, we visualize the self-similarity matrices of the Museformer demos on <a href="https://museformer.github.io/">the demo page</a>, to vividly show the structures, as well as our ability to generate music with variations instead of simply copying. The similarities are computed over the melody track.

                        <figure>
                            <figcaption>Demo 1</figcaption>
                            <img src="rebuttal_materials/ssm_demo/mf_demo1.png">
                        </figure>

                        <figure>
                            <figcaption>Demo 2</figcaption>
                            <img src="rebuttal_materials/ssm_demo/mf_demo2.png">
                        </figure>

                        <figure>
                            <figcaption>Demo 3</figcaption>
                            <img src="rebuttal_materials/ssm_demo/mf_demo3.png">
                        </figure>

                        <figure>
                            <figcaption>Demo 4</figcaption>
                            <img src="rebuttal_materials/ssm_demo/mf_demo4.png">
                        </figure>

                        As you may see, there are many similar bars in the demos, and the music pieces tend to repeat or imitate the previous bars at the distance of 1, 2, 4, 8, ..., which manifests reasonable structures. Note that there are also many places where the similarities are larger than 0.0 and less than 1, and it means that these bars are similar and also have variations between them. If you watch the videos on <a href="https://museformer.github.io/">the demo page</a>, you will also find that there are many variations on the melody track, let alone that in most time, the accompaniment tracks can be completely different for the similar melody. Therefore, Museformer can model both repetitions and variations, the combination of which makes good music structures of the generated music.
                        

                    </section>


                    <br /><br />
                    <p>If anything is not clear enough to you, please feel free to ask us or have a discussion
                        with us.</p>
                    <p>Thank you for everything &#128151;!</p>

                    </section>

                </article>
            </div>
            <nav class="section-nav">
                <ol>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#dataset_analysis">Similarity Distribution on Different Datasets</a>
                        <ul>
                            <li class=""><a href="#dataset_analysis_lmd">LMD (TopMAGD)</a></li>
                            <li class=""><a href="#dataset_analysis_symphony">Symphony</a></li>
                        </ul>
                    </li>
                    <li><a href="#music_analysis">Similarity Distribution on Generated Music</a></li>
                    <li><a href="#ssm">Self-Similarity Matrix of Demos</a></li>
                </ol>
            </nav>
        </main>
    </div>



</body>

</html>